{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2438197d",
   "metadata": {},
   "source": [
    "# Scraping Top Repositories for Topics on GitHub\n",
    "\n",
    "- Introduction about web scraping\n",
    "- Introduction about GitHub and the problem statement\n",
    "- Mention the tools you're using (Python, requests, Beautiful Soup, Pandas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52146167",
   "metadata": {},
   "source": [
    "# Here are the steps we'll follow:\n",
    "\n",
    "- We're going to scrape https://github.com/topics\n",
    "- We'll get a list of topics. For each topic, we'll get topic title, topic page URL and topic description\n",
    "- For each topic, we'll get the top 30 repositories in the topic from the topic page\n",
    "- For each repository, we'll grab the repo name, username, stars and repo URL\n",
    "- For each topic we'll create a CSV file in the following format:\n",
    "\n",
    "```\n",
    "Repo Name,Username,Repo URL\n",
    "three.js,mrdoob,https://github.com/mrdoob/three.js\n",
    "libgdx,libgdx,https://github.com/libgdx/libgdx\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95ffbd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\niket\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\niket\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\niket\\anaconda3\\lib\\site-packages (from requests) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\niket\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\niket\\anaconda3\\lib\\site-packages (from requests) (2022.9.14)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\niket\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\niket\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install beautifulsoup4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b7e45f",
   "metadata": {},
   "source": [
    "# Scrape the list of topics from Github\n",
    "\n",
    "Explain how you'll do it.\n",
    "\n",
    "- use requests to downlaod the page\n",
    "- user BS4 to parse and extract information\n",
    "- convert to a Pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a163fb",
   "metadata": {},
   "source": [
    "Let's write a function to download the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b78f34c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_topics_page():\n",
    "    topics_url = 'https://github.com/topics'\n",
    "    response = requests.get(topics_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f229fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = get_topics_page()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fac8cb5",
   "metadata": {},
   "source": [
    "Let's create some helper functions to parse information from the page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342b1ae8",
   "metadata": {},
   "source": [
    "`get_topic_titles` can be used to get the list of titles\n",
    "\n",
    "functions to parse information from the page.\n",
    "\n",
    "To get topic titles, we can pick `p` tags with the `class` ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "989a1e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_titles(doc):\n",
    "    selection_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "    topic_title_tags = doc.find_all('p', {'class': selection_class})\n",
    "    topic_titles = [tag.text for tag in topic_title_tags]\n",
    "    return topic_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5fd6f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = get_topic_titles(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "562e3cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3D', 'Ajax', 'Algorithm']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c034f43c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c3cf26",
   "metadata": {},
   "source": [
    "Function for descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9adec991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_desc(doc):\n",
    "    desc_selector = 'f5 color-fg-muted mb-0 mt-1'\n",
    "    topic_desc_tags = doc.find_all('p', {'class': desc_selector})\n",
    "    topic_descriptions = [tag.text.strip() for tag in topic_desc_tags]\n",
    "    return topic_descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440e2963",
   "metadata": {},
   "source": [
    "Function for urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "619a02e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_url(doc):\n",
    "    topic_link_tags = doc.find_all('a', {'class': 'no-underline flex-grow-0'})\n",
    "    topic_urls = ['https://github.com/' + tag['href'] for tag in topic_link_tags]\n",
    "    return topic_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218beece",
   "metadata": {},
   "source": [
    "Let's put this all together into a single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b283ed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics():\n",
    "    topic_url = 'https://github.com/topics'\n",
    "    response = requests.get(topic_url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    topic_dict = {\n",
    "        'Title': get_topic_titles(doc),\n",
    "        'Description': get_topic_desc(doc),\n",
    "        'Urls': get_topic_url(doc)\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(topic_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7800fc",
   "metadata": {},
   "source": [
    "# Get the top 20 repositories from a topic page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59a60f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_repo(h3_tags):\n",
    "    base_url = 'https://github.com/'\n",
    "    a_tags = h3_tags.find_all('a')\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    repo_url = base_url + a_tags[1]['href']\n",
    "    return username, repo_name, repo_url\n",
    "\n",
    "def get_topic_info(topic_url):\n",
    "    #Download the page\n",
    "    response = requests.get(topic_url)\n",
    "    \n",
    "    #check Succesfull response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    \n",
    "    #parse using BeautifulSoup\n",
    "    topic_doc = BeautifulSoup(response.text,'html.parser')\n",
    "    \n",
    "    #Get h3 tags containing information about username, Repository topis and url\n",
    "    h3_selection_tag = 'f3 color-fg-muted text-normal lh-condensed'\n",
    "    repo_tags = topic_doc.find_all('h3',{'class':h3_selection_tag})\n",
    "    \n",
    "    \n",
    "    #Get the repo together\n",
    "    topic_info_dict = { 'username':[], 'repo_name':[], 'repo_url':[]}\n",
    "    \n",
    "    \n",
    "    for i in range(len(repo_tags)):\n",
    "        repo_info = get_info_repo(repo_tags[i])\n",
    "        topic_info_dict['username'].append(repo_info[0])\n",
    "        topic_info_dict['repo_name'].append(repo_info[1])\n",
    "        topic_info_dict['repo_url'].append(repo_info[2])\n",
    "    \n",
    "    return pd.DataFrame(topic_info_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82dc4092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topic(topic_url,path):\n",
    "    if os.path.exists(path):\n",
    "        print(\"the file {} already exixsts.skipping...\".format(path))\n",
    "        return\n",
    "    topic_df = get_topic_info(topic_url)\n",
    "    topic_df.to_csv(path,index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcff830",
   "metadata": {},
   "source": [
    "# Putting it all together\n",
    "- We have a funciton to get the list of topics\n",
    "- We have a function to create a CSV file for scraped repos from a topics page\n",
    "- Create a function to put them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4287c4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics_repos():\n",
    "    print('Scraping List of topics')\n",
    "    topics_df = scrape_topics()\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    \n",
    "    for index,row in topics_df.iterrows():\n",
    "        print('Scraping top repository for \"{}\"'.format(row['Title']))\n",
    "        scrape_topic(row['Urls'],'data/{}.csv'.format(row['Title']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594f5630",
   "metadata": {},
   "source": [
    "Let's run it to scrape the top repos for the all the topics on the first page of https://github.com/topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae339bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping List of topics\n",
      "Scraping top repository for \"3D\"\n",
      "Scraping top repository for \"Ajax\"\n",
      "Scraping top repository for \"Algorithm\"\n",
      "Scraping top repository for \"Amp\"\n",
      "Scraping top repository for \"Android\"\n",
      "Scraping top repository for \"Angular\"\n",
      "Scraping top repository for \"Ansible\"\n",
      "Scraping top repository for \"API\"\n",
      "Scraping top repository for \"Arduino\"\n",
      "Scraping top repository for \"ASP.NET\"\n",
      "Scraping top repository for \"Atom\"\n",
      "Scraping top repository for \"Awesome Lists\"\n",
      "Scraping top repository for \"Amazon Web Services\"\n",
      "Scraping top repository for \"Azure\"\n",
      "Scraping top repository for \"Babel\"\n",
      "Scraping top repository for \"Bash\"\n",
      "Scraping top repository for \"Bitcoin\"\n",
      "Scraping top repository for \"Bootstrap\"\n",
      "Scraping top repository for \"Bot\"\n",
      "Scraping top repository for \"C\"\n",
      "Scraping top repository for \"Chrome\"\n",
      "Scraping top repository for \"Chrome extension\"\n",
      "Scraping top repository for \"Command line interface\"\n",
      "Scraping top repository for \"Clojure\"\n",
      "Scraping top repository for \"Code quality\"\n",
      "Scraping top repository for \"Code review\"\n",
      "Scraping top repository for \"Compiler\"\n",
      "Scraping top repository for \"Continuous integration\"\n",
      "Scraping top repository for \"COVID-19\"\n",
      "Scraping top repository for \"C++\"\n"
     ]
    }
   ],
   "source": [
    "scrape_topics_repos()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
